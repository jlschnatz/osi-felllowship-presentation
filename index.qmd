---
title: Simulation-Based Publication Bias Assessement in Meta-Analyses
subtitle: _OSI Fellowship Ceremony 2024_
format: clean-revealjs
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Luca Schnatz [{{< fa brands github >}}](https://github.com/jlschnatz) [{{< fa brands x-twitter >}}](https://x.com/jlschnatz) 
    email: schnatz@psych.uni-frankfurt.de
    affiliations: Department of Psychological Methods with Interdisciplinary Focus 
header-logo: img/logos_header.png
slide-number: true
fig-cap-location: margin
cap-location: margin
bibliography: references.bib
csl: apa.csl
callout-appearance: default
css: logo.css
callout-icon: false
filters:
  - reveal-header
date: 07-03-2024
---

## Background

### Comparing the Standard Literature against Registered Reports

Exemplifying data by @scheel2021excess

::: {style="text-align:center;font-size:0.8em; color:grey;"}

![](img/scheel_etal_2021.png){fig-align="center" width="50%" fig-alt="Data of Scheel et al. (2021) indicating that 96% of the standard literature, the first hypothesis is supported, while only 44% is supported in publication bias absent registered reports."}

:::

::: {.notes}

- To highlight the point, I want to make &rarr; here is some data from three years ago that compares standard reports against registered replication reports (in which publication bias is absent because of the publishing format)
- I read this study last year when I began this project and was shocked by the results
- Shows that there is a huge discrepancy in the percentage of studies supporting their first hypothesis depending on the publishing format
- 96% of standard reports that are published support their first hypothesis, while only 44% of registered replication reports support that &rarr; indicates widespread publication bias

:::

## Background 

### Relevance for Open Science

*Publication bias leads to...*

- Inflation of meta-analytical effect sizes [@franco2014publication]
- Increased false-positive rate [@kicinski2014does]

&rarr; Publication bias threatens the validity of science and is a significant driver of the **replication crisis** in psychology [@munafo2017manifesto]


::: {.notes}

- And the consequences of this widespread evident publication bias (as you may know) are very severe
- Publication bias may lead to inflated meta-analytical effect sizes
- Which then in return also leads to increased false-positive rate
- In summary, we may say that publication bias threatens the validity of science -> and is a significant driver of the replication crisis in psychology

:::


## Background
### Existing Methods to Assess Publication Bias in Meta-Analyses

General classification into:

- Methods based on the relationship between effect size and sample size (*small-study effects*) 
- Methods working with *p*-values

::: {.notes}

- Methods based on the relationship between effect size and sample size (also coined small-study effects)
  - When publication bias is present: studies with smaller sample sizes (lower precision) need larger effect sizes to achieve statistical significance compared to studies with larger sample sizes (higher precision)
  - As a result, there are disproportionately fewer studies with low sample sizes and low effect sizes because they are statistically nonsignificant
  - And this relationship is captured via regression-based methods 
- Methods based on p-values: Model publication bias by considering how the likelihood of a study's publication depends on its p-values

:::



## Background
### Criticisms of existing Methods

- Small-study effects methods: lack of an *explicit* model for publication bias [@mcshane2016adjusting] &rarr; *indirect* approach [@harrer2021doing]
- Publication bias not the only factor influencing the distribution of effect size and sample size &rarr; *sample size planning* [@linden2024publication]
- Existing methods perform poorly in realistic empirical scenarios [@renkewitz2019detect; @mcshane2016adjusting; @carter2019correcting]
  - Effect size heterogeneity
  - Additional *p*-hacking and other QRPs
  - Limited number of individual studies included in the meta-analysis
  - Severity of publication bias



::: {.notes}

- Despite the abundance of statistical methods to assess publication bias, many criticisms
- Small-study effects methods lack and explicit model for publication bias &rarr; only use indirect approach to assess publication bias
- Additionally, publication bias is not the only factor that influences the joint distribution of effect size and sample size &rarr; *sample size planning* &rarr; researchers planning their sample size for studies according to anticipated effect sizes &rarr; should be accounted for when modelling publication bias
- Several simulation studies have shown that existing methods perform poorly under realistic empirical scenarios &rarr; factors influencing the performance: 
  - effect size heterogeneity
  - additional p-hacking or other QRPs
  - limited number of individual studies
  - severity of publication bias

:::

## Introducing SPEEC

- Explicit and modeling framework to assess publication bias with ability to account for relevant discussed factors in the context of publication bias
  - heterogeneity, 
  - *sample size planning*, 
  - *p*-hacking
- Adaptability and flexibility in assumptions for different scenarios
- *SPEEC*: **S**imulation-based **P**ublication Bias **E**stimation and **E**ffect Size **C**orrection &rarr; written as open source [R-package](https://github.com/jlschnatz/speec) 

::: {.notes}

- Considering these limitations, an explicit modelling framework to assess publication bias with the ability to account important influencing factors in the context of publication bias that I have talked about would be valuable: effect size heterogeneity, sample size planning, *p*-hacking
- Because as we include more and more factors to model publication bias, the distribution of effect size and sample size gets harder to describe analytically, we came up with the idea to just use simulation!
- I called this method SPEEC, which stands for simulation-based publication biuas estimation and effect size correction &rarr; two factors
  - Quantification of the extent of publication bias -> how much more likely statistically significant studies are to be published compared to nonsignificant studies
  - Correction of effect sizes for the impact of publication bias
- I have written this framework as an open-source R package (still an alpha version, will need to change a lot)

:::

## Flowchart of SPEEC

![](img/speec_workflow.png){fig-align="center" width="110%"}

::: {.notes}

- Main idea: explicit modelling of the generative process of publication bias in a simulation framework 
- Iteratively compare simulated data from publication bias model to empirical meta-analytical effect size sample size data &rarr; to quantify the closeness of the simulated and empirical data
- Adjust parameter of the simulation framework statistical distance of the joint distributions is maximized

:::

## Preliminary Assessement of SPEEC

- Proof of concept study
- Utilisation of secondary data [@linden2021heterogeneity] with 150 classical meta-analyses and 57 registered replication reports
- Confirmatory results suggested problems within the parameter estimation of SPEEC
- Exploratory results aimed to identify potential root causes of estimation problems
  - Unaccounted between-study effect size heterogeneity in the simulation framework

## Outlook

### Nexts Steps

- Account for between-study effect size heterogeneity in the simulation framework
- Conduct comprehensive simulation study to assess SPEEC
- Include other factors that are influencing the joint distribution of effect size and sample size (such as sample size planning, *p*-hacking, etc.)

### Interested in More Details?

- [Bachelor thesis](https://raw.githubusercontent.com/jlschnatz/bachelor-thesis/master/manuscript/thesis-schnatz_submitted.pdf)
- [Project code](https://github.com/jlschnatz/bachelor-thesis)

## Acknowledgments

Finally, I want to deeply thank Martin and Julia who supported me along the way of this project!

## References
